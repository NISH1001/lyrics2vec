{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import re\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(texts):\n",
    "    if type(texts) is str:\n",
    "        texts = [texts]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        return sess.run(embed(texts))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', 'be', 'academy', 'locate']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(stop_words, tokens):\n",
    "    res = []\n",
    "    for token in tokens:\n",
    "        if not token in stop_words:\n",
    "            res.append(token)\n",
    "    return res\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.encode('ascii', errors='ignore').decode()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r'#+', ' ', text )\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub(r\"([A-Za-z]+)'s\", r\"\\1 is\", text)\n",
    "    #text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"won't\", \"will not \", text)\n",
    "    text = re.sub(r\"isn't\", \"is not \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma_list = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, 'v')\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "        lemma_list.append(lemma)\n",
    "    # return [ lemmatizer.lemmatize(token, 'v') for token in tokens ]\n",
    "    return lemma_list\n",
    "\n",
    "\n",
    "def process_all(text):\n",
    "    text = process_text(text)\n",
    "    return ' '.join(remove_stopwords(stop_words, text.split()))\n",
    "\n",
    "lemmatize(\"where is academy located \".split())\n",
    "#process_all(\"location of academy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lyrics import LyricFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = [\n",
    "    \"the man who sold the world nirvana\",\n",
    "    \"smells like teen spirit\",\n",
    "    \"rape me nirvana\",\n",
    "    \"come as you are nivrana\",\n",
    "    \"about a girl nirvana\",\n",
    "    \"love buzz nirvana\",\n",
    "    \"where did you sleep last night nirvana\",\n",
    "    \"american idiot green day\",\n",
    "    \"time of your life green day\",\n",
    "    \"boulevard of broken dreams green day\",\n",
    "    \"wake me up when september ends green day\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfinder = LyricFinder()\n",
    "lyrics = []\n",
    "songs_found = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man who sold the world nirvana\n",
      "Searching lyrics.wikia.com\n",
      "smells like teen spirit\n",
      "Searching lyrics.wikia.com\n",
      "rape me nirvana\n",
      "Searching lyrics.wikia.com\n",
      "come as you are nivrana\n",
      "Searching lyrics.wikia.com\n",
      "about a girl nirvana\n",
      "Searching lyrics.wikia.com\n",
      "love buzz nirvana\n",
      "Searching lyrics.wikia.com\n",
      "where did you sleep last night nirvana\n",
      "Searching lyrics.wikia.com\n",
      "american idiot green day\n",
      "Searching lyrics.wikia.com\n",
      "time of your life green day\n",
      "Searching lyrics.wikia.com\n",
      "boulevard of broken dreams green day\n",
      "Searching lyrics.wikia.com\n",
      "wake me up when september ends green day\n",
      "Searching lyrics.wikia.com\n"
     ]
    }
   ],
   "source": [
    "for song in songs:\n",
    "    print(song)\n",
    "    lyric = lfinder.search(song)\n",
    "    if lyric:\n",
    "        lyrics.append(process_text(lyric))\n",
    "        songs_found.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lyrics) == len(songs_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "lyric2vec = get_features(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 512)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = cosine_similarity(lyric2vec, lyric2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['boulevard of broken dreams green day'],\n",
       "       ['american idiot green day'],\n",
       "       ['wake me up when september ends green day'],\n",
       "       ['come as you are nivrana'],\n",
       "       ['where did you sleep last night nirvana'],\n",
       "       ['boulevard of broken dreams green day'],\n",
       "       ['smells like teen spirit'],\n",
       "       ['the man who sold the world nirvana'],\n",
       "       ['the man who sold the world nirvana'],\n",
       "       ['the man who sold the world nirvana']], dtype='<U40')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(songs_found)[np.flip(np.argsort(sims, axis=1), axis=1)[:, 1:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_song(query, songs, lyric2vec):\n",
    "    query = process_text(query)\n",
    "    query_vec = get_features(query)\n",
    "    res = []\n",
    "    scores = cosine_similarity(query_vec, lyric2vec).ravel()\n",
    "    sorted_idx = np.argsort(scores)[::-1]\n",
    "    return list(zip(np.array(songs)[sorted_idx], scores[sorted_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('come as you are nivrana', 0.23251113),\n",
       " ('smells like teen spirit', 0.23131013),\n",
       " ('love buzz nirvana', 0.18689522),\n",
       " ('about a girl nirvana', 0.17386755),\n",
       " ('rape me nirvana', 0.14984597),\n",
       " ('time of your life green day', 0.1436481),\n",
       " ('american idiot green day', 0.12091069),\n",
       " ('boulevard of broken dreams green day', 0.118268244),\n",
       " ('where did you sleep last night nirvana', 0.10211811),\n",
       " ('the man who sold the world nirvana', 0.091701984)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_song(\"she is overboard selfish\", songs, lyric2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

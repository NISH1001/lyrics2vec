{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import re\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder\n",
    "It is the model for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The model is efficient and result in accurate performance on diverse transfer tasks.  \n",
    "#### References\n",
    "- [arxiv](https://arxiv.org/abs/1803.11175)\n",
    "- [tensorflow hub](https://tfhub.dev/google/universal-sentence-encoder-large/3)\n",
    "- [colab notebook](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embedding Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensroflow hub module for Universal sentence Encoder\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor\n",
    "This is just a simple function to wrap tensorflow call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(texts):\n",
    "    if type(texts) is str:\n",
    "        texts = [texts]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        return sess.run(embed(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Textual Shit\n",
    "Remove unnecessary characters, stopwords, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rape me'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(stop_words, tokens):\n",
    "    res = []\n",
    "    for token in tokens:\n",
    "        if not token in stop_words:\n",
    "            res.append(token)\n",
    "    return res\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.encode('ascii', errors='ignore').decode()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r'#+', ' ', text )\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub(r\"([A-Za-z]+)'s\", r\"\\1 is\", text)\n",
    "    #text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"won't\", \"will not \", text)\n",
    "    text = re.sub(r\"isn't\", \"is not \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma_list = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, 'v')\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "        lemma_list.append(lemma)\n",
    "    # return [ lemmatizer.lemmatize(token, 'v') for token in tokens ]\n",
    "    return lemma_list\n",
    "\n",
    "\n",
    "def process_all(text):\n",
    "    text = process_text(text)\n",
    "    return ' '.join(remove_stopwords(stop_words, text.split()))\n",
    "\n",
    "process_text('rape! me')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Lyrics Finder module\n",
    "I had created a simple tool to find lyrics for any songs. The github repo is:  \n",
    "https://github.com/NISH1001/AZlyrics/  \n",
    "\n",
    "I have made use of the `LyricFinder` class that is easy to import (modular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lyrics import LyricFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are some dummy song names whose lyrics are to be searched\n",
    "songs = [\n",
    "    \"the man who sold the world nirvana\",\n",
    "    \"smells like teen spirit\",\n",
    "    \"rape me nirvana\",\n",
    "    \"come as you are nivrana\",\n",
    "    \"about a girl nirvana\",\n",
    "    \"love buzz nirvana\",\n",
    "    \"where did you sleep last night nirvana\",\n",
    "    \"american idiot green day\",\n",
    "    \"time of your life green day\",\n",
    "    \"boulevard of broken dreams green day\",\n",
    "    \"wake me up when september ends green day\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the objects\n",
    "lfinder = LyricFinder()\n",
    "lyrics = []\n",
    "songs_found = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man who sold the world nirvana\n",
      "Searching lyrics.wikia.com\n",
      "smells like teen spirit\n",
      "Searching lyrics.wikia.com\n",
      "rape me nirvana\n",
      "Searching lyrics.wikia.com\n",
      "come as you are nivrana\n",
      "Searching lyrics.wikia.com\n",
      "about a girl nirvana\n",
      "Searching lyrics.wikia.com\n",
      "love buzz nirvana\n",
      "Searching lyrics.wikia.com\n",
      "where did you sleep last night nirvana\n",
      "Searching lyrics.wikia.com\n",
      "american idiot green day\n",
      "Searching lyrics.wikia.com\n",
      "time of your life green day\n",
      "Searching lyrics.wikia.com\n",
      "boulevard of broken dreams green day\n",
      "Searching lyrics.wikia.com\n",
      "wake me up when september ends green day\n",
      "Searching lyrics.wikia.com\n"
     ]
    }
   ],
   "source": [
    "for song in songs:\n",
    "    print(song)\n",
    "    lyric = lfinder.search(song)\n",
    "    if lyric:\n",
    "        lyrics.append(process_text(lyric))\n",
    "        songs_found.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lyrics) == len(songs_found) # LOL :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "Here, the vectors for lyrics are extracted using the universal sentence encoder module mentioned earlier.  \n",
    "The size of each vector is **512**.  \n",
    "So, we get a **(n, 512)** array for **n** number of songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "lyric2vec = get_features(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric2vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Nearest Song\n",
    "For each songs in the list, find the song that is nearest semantically.  \n",
    "Since Universal Sentence Encoder gives some semantic embedding of each text (lyrics), we just find the cosine similarity of each song with every other song and get the one with highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['boulevard of broken dreams green day'],\n",
       "       ['american idiot green day'],\n",
       "       ['wake me up when september ends green day'],\n",
       "       ['come as you are nivrana'],\n",
       "       ['where did you sleep last night nirvana'],\n",
       "       ['boulevard of broken dreams green day'],\n",
       "       ['smells like teen spirit'],\n",
       "       ['the man who sold the world nirvana'],\n",
       "       ['the man who sold the world nirvana'],\n",
       "       ['the man who sold the world nirvana']], dtype='<U40')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple test\n",
    "sims = cosine_similarity(lyric2vec, lyric2vec)\n",
    "np.array(songs_found)[np.flip(np.argsort(sims, axis=1), axis=1)[:, 1:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Semantic Search\n",
    "Here, I create a simple function that accepts a query text and returns the list of songs sorted in semantically strong score.  \n",
    "Higher the score, higher is the semantic between the query and the song lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_song(query, songs, lyric2vec):\n",
    "    query = process_text(query)\n",
    "    query_vec = get_features(query)\n",
    "    res = []\n",
    "    scores = cosine_similarity(query_vec, lyric2vec).ravel()\n",
    "    sorted_idx = np.argsort(scores)[::-1]\n",
    "    return list(zip(np.array(songs)[sorted_idx], scores[sorted_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('come as you are nivrana', 0.23251113),\n",
       " ('smells like teen spirit', 0.23131013),\n",
       " ('love buzz nirvana', 0.18689522),\n",
       " ('about a girl nirvana', 0.17386755),\n",
       " ('rape me nirvana', 0.14984597),\n",
       " ('time of your life green day', 0.1436481),\n",
       " ('american idiot green day', 0.12091069),\n",
       " ('boulevard of broken dreams green day', 0.118268244),\n",
       " ('where did you sleep last night nirvana', 0.10211811),\n",
       " ('the man who sold the world nirvana', 0.091701984)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_song(\"she is overboard selfish\", songs, lyric2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
